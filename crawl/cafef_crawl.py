assert False 
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
from selenium.common.exceptions import NoSuchElementException
import urllib.parse
import time
import csv

import unicodedata

def to_slug(text):
    # BÆ°á»›c 1: chuáº©n hÃ³a Unicode (NFD tÃ¡ch dáº¥u khá»i chá»¯ cÃ¡i)
    text = unicodedata.normalize('NFD', text)
    # BÆ°á»›c 2: loáº¡i bá» dáº¥u (chá»¯ cÃ³ dáº¥u sáº½ bá»‹ tÃ¡ch thÃ nh 2 pháº§n: base + dáº¥u, ta loáº¡i dáº¥u)
    text = ''.join([c for c in text if unicodedata.category(c) != 'Mn'])
    # BÆ°á»›c 3: chuyá»ƒn thÃ nh chá»¯ thÆ°á»ng vÃ  bá» khoáº£ng tráº¯ng
    text = text.lower().replace(' ', '')
    return text

def get_article_data(driver, url):
    print(f"â¡ï¸ Truy cáº­p bÃ i viáº¿t: {url}")
    driver.get(url)
    time.sleep(2.5)

    # Láº¥y tiÃªu Ä‘á»
    try:
        title = driver.find_element(By.CSS_SELECTOR, "h1.title[data-role='title']").text.strip()
        print(f"ğŸ“ TiÃªu Ä‘á»: {title}")
    except:
        print("âŒ KhÃ´ng láº¥y Ä‘Æ°á»£c tiÃªu Ä‘á»")
        title = ""

    # Láº¥y ngÃ y Ä‘Äƒng
    try:
        date = driver.find_element(By.CSS_SELECTOR, "span.pdate[data-role='publishdate']").text.strip()
        print(f"ğŸ“… NgÃ y Ä‘Äƒng: {date}")
    except:
        print("âŒ KhÃ´ng láº¥y Ä‘Æ°á»£c ngÃ y Ä‘Äƒng")
        date = ""

    return title, date

def crawl_cafef_articles(keyword, max_pages=5, output_csv="cafef_output.csv"):
    output_csv = to_slug(keyword)+"_"+output_csv
    print(f"ğŸ” Báº¯t Ä‘áº§u tÃ¬m kiáº¿m tá»« khÃ³a: {keyword}")
    options = Options()
    options.add_argument("--headless")
    driver = webdriver.Chrome(options=options)

    query = urllib.parse.quote(keyword)
    all_links = []

    # Duyá»‡t qua tá»«ng trang tÃ¬m kiáº¿m
    for page in range(1, max_pages + 1):
        search_url = f"https://cafef.vn/tim-kiem/trang-{page}.chn?keywords={query}"
        print(f"\nğŸ“„ Äang truy cáº­p: {search_url}")
        driver.get(search_url)
        time.sleep(2.5)

        try:
            articles = driver.find_elements(By.XPATH, "//div[@class='item']")
            print(f"ğŸ”— TÃ¬m tháº¥y {len(articles)} bÃ i viáº¿t á»Ÿ trang {page}")
            for i, article in enumerate(articles, 1):
                try:
                    a_tag = article.find_element(By.TAG_NAME, "a")
                    link = a_tag.get_attribute("href")
                    if link and link.startswith("https://"):
                        all_links.append(link)
                        print(f"   [{i}] âœ… Link: {link}")
                except:
                    print(f"   [{i}] âŒ KhÃ´ng láº¥y Ä‘Æ°á»£c link")
        except Exception as e:
            print(f"âš ï¸ Lá»—i khi láº¥y bÃ i viáº¿t á»Ÿ trang {page}: {e}")

    print(f"\nâœ… Tá»•ng cá»™ng {len(all_links)} bÃ i viáº¿t Ä‘Æ°á»£c thu tháº­p.\n")

    # Ghi vÃ o file CSV
    with open(output_csv, "w", newline="", encoding="utf-8") as csvfile:
        writer = csv.writer(csvfile)
        writer.writerow(["Title", "Date"])

        for idx, link in enumerate(all_links, 1):
            print(f"\nğŸ“¥ [{idx}/{len(all_links)}] Äang xá»­ lÃ½ bÃ i viáº¿t...")
            try:
                title, date = get_article_data(driver, link)
                writer.writerow([title, date])
            except Exception as e:
                print(f"âš ï¸ Lá»—i khi xá»­ lÃ½ bÃ i viáº¿t: {e}")
                continue

    driver.quit()
    print(f"\nğŸ“ Dá»¯ liá»‡u Ä‘Ã£ Ä‘Æ°á»£c lÆ°u vÃ o: {output_csv}")
    time.sleep(10)

# âœ… Gá»i hÃ m chÃ­nh
# crawl_cafef_articles("SÆ¡n TÃ¹ng", max_pages=5)
crawl_cafef_articles("Tráº¥n ThÃ nh", max_pages=5)
crawl_cafef_articles("ÄÃ´ng Nhi", max_pages=5)
crawl_cafef_articles("SÆ¡n TÃ¹ng", max_pages=5)
crawl_cafef_articles("Há»“ Ngá»c HÃ ", max_pages=5)
crawl_cafef_articles("XuÃ¢n Báº¯c", max_pages=5)
crawl_cafef_articles("Ãi PhÆ°Æ¡ng", max_pages=5)
crawl_cafef_articles("Minh Háº±ng", max_pages=5)
crawl_cafef_articles("Khá»Ÿi My", max_pages=5)
crawl_cafef_articles("NgÃ´ Thanh VÃ¢n", max_pages=5)
crawl_cafef_articles("Midu", max_pages=5)
# crawl_cafef_articles("SÆ¡n TÃ¹ng", max_pages=5)