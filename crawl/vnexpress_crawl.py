from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
from selenium.common.exceptions import NoSuchElementException, ElementClickInterceptedException
import urllib.parse
import time
import csv

import unicodedata

def to_slug(text):
    # B∆∞·ªõc 1: chu·∫©n h√≥a Unicode (NFD t√°ch d·∫•u kh·ªèi ch·ªØ c√°i)
    text = unicodedata.normalize('NFD', text)
    # B∆∞·ªõc 2: lo·∫°i b·ªè d·∫•u (ch·ªØ c√≥ d·∫•u s·∫Ω b·ªã t√°ch th√†nh 2 ph·∫ßn: base + d·∫•u, ta lo·∫°i d·∫•u)
    text = ''.join([c for c in text if unicodedata.category(c) != 'Mn'])
    # B∆∞·ªõc 3: chuy·ªÉn th√†nh ch·ªØ th∆∞·ªùng v√† b·ªè kho·∫£ng tr·∫Øng
    text = text.lower().replace(' ', '')
    return text

def get_article_data(driver, url):
    driver.get(url)
    time.sleep(4)

    # L·∫•y title
    try:
        title = driver.find_element(By.CSS_SELECTOR, "h1.title-detail").text.strip()
    except:
        title = ""

    # L·∫•y ng√†y ƒëƒÉng
    try:
        date = driver.find_element(By.XPATH, "//span[@class='date']").text.strip()
    except:
        date = ""

    # M·ªü r·ªông t·∫•t c·∫£ c√°c b√¨nh lu·∫≠n n·∫øu c√≥ n√∫t "Xem th√™m b√¨nh lu·∫≠n"
    while True:
        try:
            btn = driver.find_element(By.XPATH, "//a[@id='show_more_coment']")
            driver.execute_script("arguments[0].click();", btn)
            time.sleep(1.5)
        except (NoSuchElementException, ElementClickInterceptedException):
            break

    # L·∫•y danh s√°ch comment
    comments = []
    try:
        comment_elements = driver.find_elements(By.XPATH, "//p[@class='full_content']")
        for cmt in comment_elements:
            text = cmt.text.strip()
            if text:
                comments.append(text)
    except:
        pass

    comment_text = "|||".join(comments)
    return title, date, comment_text

def crawl_vnexpress_articles(keyword, max_pages=5, output_csv="vnexpress_output.csv"):
    output_csv = to_slug(keyword)+"_"+output_csv
    options = Options()
    options.add_argument("--headless")
    driver = webdriver.Chrome(options=options)

    base_url = "https://timkiem.vnexpress.net/"
    query = urllib.parse.quote(keyword)
    all_links = []

    # Crawl c√°c link t·ª´ trang t√¨m ki·∫øm
    for page in range(1, max_pages + 1):
        search_url = (
            f"{base_url}?q={query}"
            f"&media_type=all&fromdate=0&todate=0&latest=&cate_code="
            f"&search_f=title,tag_list&date_format=all&page={page}"
        )
        driver.get(search_url)
        time.sleep(2)

        article_elements = driver.find_elements(By.XPATH, "//h3[contains(@class, 'title-news')]/a")
        for a in article_elements:
            link = a.get_attribute("href")
            if link and link.startswith("https://"):
                all_links.append(link)

    print(f"‚úÖ T·ªïng c·ªông {len(all_links)} b√†i vi·∫øt ƒë∆∞·ª£c thu th·∫≠p")

    # Ghi d·ªØ li·ªáu v√†o CSV
    with open(output_csv, "w", newline="", encoding="utf-8") as csvfile:
        writer = csv.writer(csvfile)
        writer.writerow(["Title", "Date", "Comments"])

        for idx, link in enumerate(all_links, 1):
            print(f"üîé [{idx}/{len(all_links)}] ƒêang x·ª≠ l√Ω: {link}")
            try:
                title, date, comments = get_article_data(driver, link)
                writer.writerow([title, date, comments])
            except Exception as e:
                print(f"‚ö†Ô∏è L·ªói v·ªõi b√†i vi·∫øt: {link} ‚Äì {e}")
                continue

    driver.quit()
    print(f"‚úÖ D·ªØ li·ªáu ƒë√£ ƒë∆∞·ª£c l∆∞u v√†o file: {output_csv}")

# ‚úÖ G·ªçi h√†m ch√≠nh v·ªõi t·ª´ kh√≥a v√≠ d·ª•
# crawl_vnexpress_articles("S∆°n T√πng", max_pages=5)
crawl_vnexpress_articles("Tr·∫•n Th√†nh", max_pages=5)
crawl_vnexpress_articles("ƒê√¥ng Nhi", max_pages=5)
crawl_vnexpress_articles("S∆°n T√πng", max_pages=5)
crawl_vnexpress_articles("H·ªì Ng·ªçc H√†", max_pages=5)
crawl_vnexpress_articles("Xu√¢n B·∫Øc", max_pages=5)
crawl_vnexpress_articles("√Åi Ph∆∞∆°ng", max_pages=5)
crawl_vnexpress_articles("Minh H·∫±ng", max_pages=5)
crawl_vnexpress_articles("Kh·ªüi My", max_pages=5)
crawl_vnexpress_articles("Ng√¥ Thanh V√¢n", max_pages=5)
crawl_vnexpress_articles("Midu", max_pages=5)
# crawl_vnexpress_articles("S∆°n T√πng", max_pages=5)